import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import brentq

# Shannon entropy for binary distribution
def H(p):
    if p == 0 or p == 1:
        return 0
    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)

# Inverse of Shannon entropy for values less than 0.5 (approximation via root finding)
def H_inverse(H_val):
    return brentq(lambda x: H(x) - H_val, 1e-10, 0.5)

# Parameters for Bernoulli distributions
a = 0.3  # a <= 0.5
p1 = 0.2  # p1 <= 0.5

# Calculating b and C1
b = min((a - p1) / (1 - 2 * p1), 1 - (a - p1) / (1 - 2 * p1))
C = H(p1) * 0.7  # Assuming C within [0, H(p1)]
C1 = (H_inverse(C) - p1) / (1 - 2 * p1)

# Generate a range of distortion values D
D_vals = np.linspace(0, 1, 1000)

# Compute R(D,C) based on given cases
R_vals = []
for D in D_vals:
    if D >= C1 and C1 <= b:
        R_vals.append(H(b) - H(C1))
    elif D < C1 and D <= b:
        R_vals.append(H(b) - H(D))
    elif min(D, C1) > b:
        R_vals.append(0)

# Plot the results
plt.figure(figsize=(8, 6))
plt.plot(D_vals, R_vals, label=f'C = {C:.2f}', color='b')
plt.xlabel('Distortion D')
plt.ylabel('R(D, C)')
plt.title('Information Rate-Distortion-Classification Function')
plt.legend()
plt.grid(True)
plt.show()
